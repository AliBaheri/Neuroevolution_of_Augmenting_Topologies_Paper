% ######################################################################################################################

\documentclass[journal, a4paper]{IEEEtran}

\usepackage{graphicx}       % For graphics, photos, etc
\usepackage{hyperref}       % For URL and href
\usepackage{amsmath}        % For advanced mathematical formatting and symbols
\usepackage{blindtext}      % For placeholder text
\usepackage{listings}       % For code listings
\usepackage{color}          % For color
\usepackage{draftwatermark} % For watermark

\graphicspath{{./illustrations/}}

\definecolor{green}{rgb}{0, 0.66, 0}
\definecolor{red}{rgb}{1, 0, 0}
\definecolor{gray}{rgb}{0.5, 0.5, 0.5}
\definecolor{orange}{rgb}{1, 0.66, 0}
\definecolor{codebg}{rgb}{0.97, 0.97, 0.97}

\newcommand{\customincludegraphics}[3]{
    \begin{figure}
        \includegraphics[width=0.45\textwidth]{{#1}}
        \caption{{#2}}
        \label{{#3}}
    \end{figure}
}
 
\lstdefinestyle{c-style}{
  language={[ANSI]C},
  frame=single,
  backgroundcolor=\color{codebg},
  commentstyle=\itshape\color{green},
  keywordstyle=\color{blue},
  numberstyle=\tiny\color{gray},
  stringstyle=\color{orange},
  basicstyle=\fontsize{7}{7}\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  captionpos=b,
  keepspaces=true,
  numbers=left,
  numbersep=5pt,
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2
}

% ######################################################################################################################

\begin{document}

\title{Neuroevolution of Augmenting Topologies}
\author{Paul Pauls\\
        Advisor: Michael Adam}
\markboth{Neuroevolution of Augmenting Topologies}{}
\maketitle

% Place small Watermark indicating that this is currently a draft in background
\SetWatermarkText{DRAFT}
\SetWatermarkScale{0.5}

% While Paper is in development shall I include this table of contents as a quick overview
\tableofcontents

\begin{abstract}
    \blindtext
\end{abstract}

% ######################################################################################################################

\section{Introduction}

\IEEEPARstart{T}{his} shall be my introduction. And this shall be my citation \cite{cite01}.
\blindtext



% ######################################################################################################################

\section{Neuroevolution and Evolutionary Algorithms}

Neuroevolution is a form of evolutionary algorithm that generates specific artificial neural networks (short form: ANN) through modification of its parameters, topology and rules in order to maximize the ANN's accuracy or fitness score. The neuroevolution algorithm seeks to modify the ANN in an evolutionary process similar to the Darwinian process that produced human brains and its process-summarizing maxim "Survival of the fittest". First methods using neuroevolution can be traced back to the 1980s and 1990s \href{https://www.inovex.de/blog/neuroevolution/}{(cite)}, though the first evolutionary algorithms were conceived in the 1950s by Alan Turing and Nils Barricelli. \href{https://en.wikipedia.org/wiki/Genetic_algorithm#cite_note-mind.oxfordjournals.org-33}{(cite)}



\subsection{Evolutionary Algorithms}


\subsection{Neuroevolution}


\subsection{Landmark Research in Neuroevolution}
% https://www.oreilly.com/ideas/neuroevolution-a-different-kind-of-deep-learning
% Past and Current Research in Neuroevolution (Also specify the explicit Neuroevolution algorithms when getting to the specific research (NEAT, HyperNEAT, DeepNeurevolution, etc))
% Include comparison of landmark research with other Reinforcement Learning Techniques (Deep Q-Learning, etc). I am thinking here especiall about Real, et al 2019
% Check out the research done by Uber-Research








% ######################################################################################################################

\section{NeuroEvolution of Augmenting Topologies (NEAT)}

<Section Introduction>

Neuroevolution of Augmenting Topologies (short form: NEAT) was first introduced in the paper "Evolving Neural Networks through Augmenting Topologies" by Kenneth O.Stanley and Risto Miikkulainen in the year 2002. [cite] It was finalized and shown to be superior to any preceding neuroevolution algorithm in Stanley's PhD thesis "Efficient Evolution of Neural Networks through Complexification" in 2004. [cite]



At time of envisioning of NEAT was Neuroevolution most promising learning approach. Still is powerful today (see rea17/19)
"NE is a promising approach to learning behavioral policies and finds solutions faster than leading RL methods on many benchmark tasks (Gomez 2003; Moriarty and Miikkulainen 1997)" \cite{sta04}


"In highly complex domains the heuristics for determining the appropriate size are not very useful, and it becomes increasingly difficult to solve such domains with fixed-length encodings." \cite{sta04}

[See all notes write down about stanleys PhD thesis]




\subsection{Key Aspects of NEAT and Differences to Preceding Neuroevolution}
% Solving Competing Convetions, Speciation, Historical Markings, Minimal Initial Pop, See Key Elements identified through ablation (http://nn.cs.utexas.edu/downloads/papers/stanley.cec02.pdf)

\subsection{Performance of NEAT}
% Include traditional NEAT performance but also NEAT performance in more current examples

\subsection{Variants and Advancements of NEAT}
% Follow Up Research and Variants (HyperNeat, ES-HyperNeat, Novelty Search, but also small variations of NEAT such as aging NEAT or rtNEAT)
% Mention non-mating variants of neuro evolution algorithms /neat (such as the current research in rea17/19). See chapter 2.3.2 'non-mating' in [sta04] for an overview

\subsubsection{<Variant 1>}
\subsubsection{<Variant 2>}
\subsubsection{<Variant 3>}



% ######################################################################################################################

\section{Practical Applications of NEAT}
% Go especially into detail what Stanley considered great NEAT applications in his reddit AMA
% Introduce my own code accompanying this paper

\subsection{<Application 1>}
\subsection{<Application 2>}
\subsection{<Application 3>}




% ######################################################################################################################

\section{Conclusion}

\blindtext



% ######################################################################################################################

\begin{thebibliography}{5}

  \bibitem{yao99}
    Yao - Evolving Artificial Neural Networks; 1999;
    \url{http://avellano.fis.usal.es/~lalonso/compt_soft/articulos/yao99evolving.pdf}

  \bibitem{sta02_1}
    Stanley, Miikkulainen - Efficient Evolution of Neural Network Topologies; 2002;
    \url{http://nn.cs.utexas.edu/downloads/papers/stanley.cec02.pdf}

  \bibitem{sta02_2}
    Stanley, Miikkulainen - Evolving Neural Networks through Augmented Topologies; 2002;
    \url{http://nn.cs.utexas.edu/downloads/papers/stanley.ec02.pdf}

  \bibitem{gea03}
    Geard, Wiles - Structure and Dynamics of a Gene Network Model Incorporating Small RNAs; Dec 2003;
    \url{https://ieeexplore.ieee.org/document/1299575}

  \bibitem{sta04}
    Stanley - Efficient Evolution of Neural Networks through Complexification; Aug 2004;
    \url{http://nn.cs.utexas.edu/downloads/papers/stanley.phd04.pdf}

  \bibitem{rei07}
    Reisinger, Miikkulainen - Acquiring Evolvability through Adaptive Representations; Jul 2007;
    \url{http://nn.cs.utexas.edu/downloads/papers/reisinger.gecco07.pdf}

  \bibitem{mat07}
    Mattiussi, Duerr, et al - Center of Mass Encoding: A self-adaptive representation with adjustable redundancy for real-valued parameters; Jul 2007;
    \url{https://infoscience.epfl.ch/record/101405}

  \bibitem{flo08}
    Floreano, Duerr, et al - Neuroevolution: From Architectures to Learning; Jan 2008;
    \url{http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.182.1567}

  \bibitem{mat08}
    Mattiussi, Marbach, et al - The Age of Analog Networks; Sep 2008;
    \url{https://www.aaai.org/ojs/index.php/aimagazine/article/view/2156}

  \bibitem{sta09}
    Stanley, Dâ€™Ambrosio, et al - A Hypercube-Based Indirect Encoding for Evolving Large-Scale Neural Networks; 2009;
    \url{http://axon.cs.byu.edu/~dan/778/papers/NeuroEvolution/stanley3**.pdf}

  \bibitem{ris11}
    Risi, Stanley - Enhancing ES-HyperNEAT to Evolve More Complex Regular Neural Networks; Jul 2011;
    \url{http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.365.4332}

  \bibitem{leh11}
    Lehman, Stanley - Novelty Search and the Problem with Objectives; Oct 2011;
    \url{https://www.cs.ucf.edu/eplex/papers/lehman_gptp11.pdf}

  \bibitem{woe12}
    Woergoetter, Porr - Scholarpedia Article on 'Reinforcement Learning'; Sep 2012;
    \url{http://www.scholarpedia.org/article/Reinforcement_learning}

  \bibitem{hol12}
    Holland - Scholarpedia Article on 'Genetic Algorithms'; Oct 2012;
    \url{http://www.scholarpedia.org/article/Genetic_algorithms}

  \bibitem{fog13}
    Fogel, Fogel, et al - Scholarpedia Article on 'Evolutionary Programming'; Oct 2013;
    \url{http://www.scholarpedia.org/article/Evolutionary_programming}

  \bibitem{leh13}
    Lehman, Miikkulainen - Scholarpedia Article on 'Neuroevolution'; Oct 2013;
    \url{http://www.scholarpedia.org/article/Neuroevolution}

  \bibitem{pas14}
    Pascanu, Ganguli, et al - On the Saddle Point for Non-Convex Optimization; May 2014;
    \url{https://www.researchgate.net/publication/262452520}

  \bibitem{kim15}
    Kim, Rigazio - Deep Clustered Convolutional Kernels; Mar 2015;
    \url{https://arxiv.org/abs/1503.01824}

  \bibitem{fer16}
    Fernando, Banarse, et al - Convolution by Evolution; Jun 2016;
    \url{https://arxiv.org/abs/1606.02580}

  \bibitem{mii17}
    Miikkulainen, Liang, et al - Evolving Deep Neural Networks; Mar 2017;
    \url{https://arxiv.org/abs/1703.00548}

  \bibitem{xie17}
    Xie, Yuille - Genetic CNN; Mar 2017;
    \url{https://arxiv.org/abs/1703.01513}

  \bibitem{neg17}
    Negrinho, Gordon - DeepArchitect: Automatically Designing and Training Deep Architectures; Apr 2017;
    \url{https://arxiv.org/abs/1704.08792}

  \bibitem{rea17}
    Real, Moore, et al - Large-scale Evolution of Image Classifiers; Jun 2017;
    \url{https://arxiv.org/abs/1703.01041}

  \bibitem{sta17_neuroevolution_overview}
    Stanley - Neuroevolution: A Different Kind of Deep Learning; Jul 2017;
    \url{https://www.oreilly.com/ideas/neuroevolution-a-different-kind-of-deep-learning}

  \bibitem{bro17}
    Brock, Lim, et al - SMASH: One-Shot Model Architecture Search through HyperNetworks; Aug 2017;
    \url{https://arxiv.org/abs/1708.05344}

  \bibitem{sal17}
    Salimans, Ho - Evolution Strategies as a Scalable Alternative to Reinforcement Learning; Sep 2017;
    \url{https://arxiv.org/abs/1703.03864}

  \bibitem{jad17}
    Jaderberg, Dalibard, et al - Population Based Training of Neural Networks; Nov 2017;
    \url{https://arxiv.org/abs/1711.09846}

  \bibitem{zha17}
    Zhang, Clune, et al - On the Relationship Between the OpenAI Evolution Strategy and Stochastic Gradient Descent; Dec 2017;
    \url{https://arxiv.org/abs/1712.06564}

  \bibitem{sta17_deep_neuroevolution}
    Stanley, Clune - Welcoming the Era of Deep Neuroevolution; Dec 2017;
    \url{https://eng.uber.com/deep-neuroevolution/}

  \bibitem{liu18}
    Liu, Simonyan, et al - Hierarchical Representation for Efficient Architecture Search; Feb 2018;
    \url{https://arxiv.org/abs/1711.00436}

  \bibitem{suc18_accelerating_deep_neuroevolution}
    Such, Stanley, et al - Accelerating Deep Neuroevolution: Train Atari in Hours on a Single Personal Computer; Apr 2018;
    \url{https://eng.uber.com/accelerated-neuroevolution/}

  \bibitem{suc18_introduction_deep_neuroevolution}
    Such, Madhavan, et al - Deep Neuroevolution: Genetic Algorithms Are a Competitive Alternative for Training Deep Neural Networks for Reinforcement Learning; Apr 2018;
    \url{https://arxiv.org/abs/1712.06567}

  \bibitem{zop18}
    Zoph, Vasudevan, et al - Learning Transferable Architectures or Scalable Image Recognition; Apr 2018;
    \url{https://arxiv.org/abs/1707.07012}

  \bibitem{leh18_evolution_strategy}
    Lehman, Chen, et al - ES Is More Than Just a Traditional Finite-Difference Approximator; May 2018;
    \url{https://arxiv.org/abs/1712.06568}

  \bibitem{leh18_safe_mutations}
    Lehman, Chen, et al - Safe Mutations for Deep and Recurrent Neural Networks through Output Gradients; May 2018;
    \url{https://arxiv.org/abs/1712.06563}

  \bibitem{zho18}
    Zhong, Yan, et al - Practical Block-Wise Neural Network Architecture Generation; May 2018;
    \url{https://arxiv.org/abs/1708.05552}

  \bibitem{raw18}
    Rawal, Miikkulainen - From Nodes to Networks: Evolving Recurrent Neural Networks; Jun 2018;
    \url{https://arxiv.org/abs/1803.04439}

  \bibitem{con18}
    Conti, Madhavan, et al - Improving Exploration in Evolution Strategies for Deep Reinforcement Learning via a Population of Novelty Seeking Agents; Oct 2018;
    \url{https://arxiv.org/abs/1712.06560}

  \bibitem{rea19}
    Real, Aggarwal, et al - Regularized Evolution for Image Classifier Architecture Search; Feb 2019;
    \url{https://arxiv.org/abs/1802.01548}

  \bibitem{sun19}
    Sun, Xue, et al - Evolving Deep Convolutional Neural Networks for Image Classification; Mar 2019;
    \url{https://arxiv.org/abs/1710.10741}



\end{thebibliography}

\end{document}
