% ######################################################################################################################

\documentclass[journal, a4paper]{IEEEtran}

\usepackage{graphicx}       % For graphics, photos, etc
\usepackage{hyperref}       % For URL and href
\usepackage{amsmath}        % For advanced mathematical formatting and symbols
\usepackage{blindtext}      % For placeholder text
\usepackage{listings}       % For code listings
\usepackage{color}          % For color
\usepackage{draftwatermark} % For watermark

\graphicspath{{./images/}}

\definecolor{green}{rgb}{0, 0.66, 0}
\definecolor{red}{rgb}{1, 0, 0}
\definecolor{gray}{rgb}{0.5, 0.5, 0.5}
\definecolor{orange}{rgb}{1, 0.66, 0}
\definecolor{codebg}{rgb}{0.97, 0.97, 0.97}

\newcommand{\customincludegraphics}[3]{
    \begin{figure}
        \includegraphics[width=0.45\textwidth]{{#1}}
        \caption{{#2}} 
        \label{{#3}}
    \end{figure}
}
 
\lstdefinestyle{c-style}{
  language={[ANSI]C},
  frame=single,
  backgroundcolor=\color{codebg},
  commentstyle=\itshape\color{green},
  keywordstyle=\color{blue},
  numberstyle=\tiny\color{gray},
  stringstyle=\color{orange},
  basicstyle=\fontsize{7}{7}\ttfamily,
  breakatwhitespace=false,
  breaklines=true,
  captionpos=b,
  keepspaces=true,
  numbers=left,
  numbersep=5pt,
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2
}

% ######################################################################################################################

\begin{document}

\title{Neuroevolution of Augmenting Topologies}
\author{Paul Pauls\\
        Advisor: Michael Adam}
\markboth{Neuroevolution of Augmenting Topologies}{}
\maketitle
  
% Place small Watermark indicating that this is currently a draft in background  
\SetWatermarkText{DRAFT}
\SetWatermarkScale{0.5}

% While Paper is in development shall I include this table of contents as a quick overview
\tableofcontents

\begin{abstract}
    \blindtext
\end{abstract}

% ######################################################################################################################

\section{Introduction}

\IEEEPARstart{T}{his} shall be my introduction. And this shall be my citation \cite{cite01}.
\blindtext



% ######################################################################################################################

\section{Neuroevolution and Evolutionary Algorithms}
% Check out the research done by Uber-Research
Neuroevolution  is  a  machine  learning  technique  that  applies  evolutionary   algorithms   to  construct artificial  neural networks,  taking  inspiration  from  the  evolution  of  biological nervous systems in nature. \cite{cite02}

A evolutionary algorithm is a generic population-based and meta-heuristically optimized algorithmic solution to an applied problem. When breaking this down to simpler terms, does this mean first of all that an evolutionary algorithm (short form: \textbf{EA}) is solution to an applied problem. This solution can take the form of a classical calculation algorithm or more complex forms like the aforementioned artificial neural network. As generic as its method of solving the problem it is applied to is its application domain. Evolutionary Algorithms - as well as Neuroevolution by which they are employed - are highly general and allow for learning without explicit targets even if provided with only minimal feedback. \cite{cite02}

Evolutionary Algorithms are population-based, meaning they not only handle a single solution to the problem they are applied to, but they have a multitude of solutions to this problem. Each of the solutions is called a \textit{member} of its arbitrarily large population and each solution is arbitrarily similar to one another. Each member of the population is judged by a common \textit{fitness function}, which numerically expresses the members quality of its solution to the applied problem. The higher the corresponding \textit{fitness function} score of a member, the better is the solution to the applied problem. All members are judged by the same fitness function, which is the key hyperparameter that determines how well a problem has been solved.

The key aspect of evolutionary algorithms however lies in its meta-heuristic optimization method. This optimization method improves the members of the population in the sense that along the evolutionary process almost all members of the population score an increasingly higher fitness function evaluation score - meaning they get increasingly better at solving the problem they are applied to.
This optimization method is employed after each \textit{generation} of a population. A generation in the evolutionary process is completed once every member of the population has been applied to the problem and has been assigned a fitness score by the common fitness function. The population is then mixed up through \textit{reproduction}, \textit{mutation}, \textit{recombination} and \textit{selection}. These processes spread traits of high-performing members to low-performing members, preserve high-performing members while extinguishing low-performing members and introduce novel traits to already high-performing members to further explore the solution-space.

Applying the attributes of Evolutionary Algorithms to artificial Neural Networks does result in the machine learning technique Neuroevolution.
% Describe EAs specific to neural networks
% Describe the 'and with arbitrary neural models and network structures.'
% Describe how it can then be classified as the ml paradigm of RL











, that . This population consists of single members which are often algorithms - or neural networks in the case of Neuroevolution - that are trying to solve the problem upon which the evolutionary algorithm is applied to. The evolutionary algorithm then aims to optimize the members of its population by maximizing their result on the \textit{fitness function} upon which all members of the population are judged and usually does so by the means of reproduction, mutation, recombination and selection - mirroring biological evolution.

This algorithmic form of \textit{natural selection} by only letting the most fit algorithms (members) sustain in the population and eradicating the less performant algorithms is a form of maximizing the cumulative reward of the whole population and does therefore classify as the machine learning paradigm of \textit{Reinforcement Learning}.

Compared to other neural network learning methods, neuroevolution is highly general; it allows learning without explicit targets, with only sparse feedback, \textbf{and with arbitrary neural models and network structures.}


% Current Research in Neuroevolution



% Explicit Neuroevolution algorithms (NEAT, etc)



% Comparison with other Reinforcement Learning Techniques (Deep Q-Learning, etc)



% ######################################################################################################################

\section{NeuroEvolution of Augmenting Topologies (NEAT)}

\subsection{<Section Introduction>}

\subsection{Key Aspects of NEAT and Differences to Preceding Neuroevolution}
% Speciation, Historical Markings, Minimal Initial Pop, See Key Elements identified through ablation (http://nn.cs.utexas.edu/downloads/papers/stanley.cec02.pdf)

\subsection{Performance of NEAT}
% NOT SURE YET IF I SHOULD KEEP THIS CHAPTER
% Chap 4 (Performance) of original Paper (http://nn.cs.utexas.edu/downloads/papers/stanley.ec02.pdf)
    
\subsection{Variants and Advancements of NEAT}
% Follow Up Research and Variants (HyperNeat, adaptive HyperNeat, etc)



% ######################################################################################################################

\section{Applications of NEAT}
% Go especially into detail what Stanley considered great NEAT applications in his reddit AMA
% Introduce my own code accompanying this paper



% ######################################################################################################################

\section{Conclusions}

\blindtext



% ######################################################################################################################

\begin{thebibliography}{5}

  \bibitem{cite01}
    Example Cite, {\em Source}, Apr. 2019.
    \url{www.example.com}
  \bibitem{cite02}
    \url{http://www.scholarpedia.org/article/Neuroevolution}

\end{thebibliography}

\end{document}
